{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfvyuHY480rEwz+BcIiZrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugeclear/study_with_chatgpt/blob/master/implement_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-head attentionを実装しよう\n"
      ],
      "metadata": {
        "id": "WJi_Mb3svBnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-6oEBrIu1jX",
        "outputId": "ce17c68e-8657-4de0-cb14-043f73b6bf64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: tensor([[ 0.0555,  0.0939],\n",
            "        [ 1.4626,  2.3343],\n",
            "        [-0.4570, -0.8459]])\n",
            "K: tensor([[-0.1360, -0.2635],\n",
            "        [ 1.1948, -1.5937],\n",
            "        [-0.3831,  1.4915]])\n",
            "scores: tensor([[-0.0323, -0.0833,  0.1188],\n",
            "        [-0.8140, -1.9728,  2.9212],\n",
            "        [ 0.2850,  0.8020, -1.0865]])\n",
            "attn: tensor([[0.3212, 0.3052, 0.3736],\n",
            "        [0.0231, 0.0073, 0.9696],\n",
            "        [0.3412, 0.5722, 0.0866]])\n",
            "output: tensor([[ 0.2101,  0.0382],\n",
            "        [-0.3863,  0.3088],\n",
            "        [ 0.6750, -0.0187]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# condition\n",
        "seq_len = 3\n",
        "d_model = 4\n",
        "d_k = 2\n",
        "d_v = 2\n",
        "\n",
        "# input (random)\n",
        "X = torch.randn(seq_len, d_model) # shape=(3,4)\n",
        "\n",
        "# definition of weighted matrix(nn.Parameter, nn.Linear is also ok)\n",
        "W_Q = torch.randn(d_model, d_k)\n",
        "W_K = torch.randn(d_model, d_k)\n",
        "W_V = torch.randn(d_model, d_v)\n",
        "\n",
        "# Q,K,Vを計算\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "# K,Qの行列積をとってソフトマックスを適用する\n",
        "scores = Q @ K.T\n",
        "\n",
        "attn = F.softmax(scores,dim=1)\n",
        "\n",
        "output = attn @ V\n",
        "\n",
        "print(f\"Q: {Q}\")\n",
        "print(f\"K: {K}\")\n",
        "print(\"scores:\", scores)\n",
        "print(\"attn:\", attn)\n",
        "print(\"output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "softmaxを使った重み付けattention出力ができるようになった"
      ],
      "metadata": {
        "id": "vebhUzFh6njn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.Functional as F\n",
        "\n",
        "seq_len = 3\n",
        "d_model = 4\n",
        "d_k = 2\n",
        "d_v = 2\n",
        "\n",
        "X = torch.Linear(seq_len, d_model)\n",
        "\n",
        "W_Q = torch.randn(d_model, d_k)\n",
        "W_K = torch.randn(d_model, d_k)\n",
        "W_V = torch.randn(d_model, d_v)\n",
        "\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "scores = ( Q @ K.T) / sqrt(d_k)\n",
        "# なぜsqrtで割る必要があるのか。\n",
        "仮にq@k_tが大き過ぎた場合、scoreも大きくなり、softmaxで行方向に正規化した際にそれぞれの数値の差が小さくなる？\n"
      ],
      "metadata": {
        "id": "NzljWPVAvDko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "def layer_norm(x, eps=1e-5):\n",
        "    # x : shape(d_model, )\n",
        "\n",
        "    mean = x.mean()\n",
        "    var = x.var(unbiased=False)\n",
        "    x_norm = (x - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "    gamma = torch.ones_like(x)\n",
        "    beta = torch.zeros_like(x)\n",
        "\n",
        "    out = x_norm * gamma + beta\n",
        "\n",
        "    return out\n",
        "\n",
        "# テスト\n",
        "x = torch.tensor([5.0, 6.0, 7.0, 8.0])\n",
        "print(layer_norm(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnnrSpjib_mY",
        "outputId": "8e1daecc-c7f1-41b5-b4ef-d27507fff002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.3416, -0.4472,  0.4472,  1.3416])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lSvcBcRjnnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer blockの実装へ\n",
        "\n",
        "Attention -> Residual -> LayerNorm -> FFN"
      ],
      "metadata": {
        "id": "NQJ-1TqOj5Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MiniTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, d_hidden):\n",
        "        super().__init__()\n",
        "\n",
        "        # Q,K,Vの線形層\n",
        "        self.W_Q = nn.Linear(d_model,d_k)\n",
        "        self.W_K = nn.Linear(d_model,d_k)\n",
        "        self.W_V = nn.Linear(d_model,d_v)\n",
        "\n",
        "        #出力 projection\n",
        "        self.W_O = nn.Linear(d_v, d_model)\n",
        "\n",
        "        # LayerNorm\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # FFN\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.GELU(), # ReLUより滑らかな活性化関数\n",
        "            nn.Linear(d_hidden, d_model)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. LayerNorm\n",
        "        h = self.ln1(x)\n",
        "\n",
        "        # 2. Q,K,V\n",
        "        Q = self.W_Q(h)\n",
        "        K = self.W_K(h)\n",
        "        V = self.W_V(h)\n",
        "\n",
        "        # 3. scaled-dot-product attention\n",
        "        scores = Q @ K.transpose(-1,-2) / math.sqrt(Q.size(-1))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        #5. Residual\n",
        "        x = x + out\n",
        "\n",
        "        h2 = self.ln2(x)\n",
        "        out2 = self.ffn(h2)\n",
        "        x = x + h2\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SSy0I1ZmfG5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = MiniTransformerBlock(d_model, d_k=2, d_v=2,d_hidden=8)\n",
        "\n",
        "X=torch.randn(3,4)\n",
        "out = block(X)\n",
        "\n",
        "print(\"入力 X:\", X)\n",
        "print(\"出力 out:\", out)\n",
        "print(\"shape:\", out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC0TAW5LkItx",
        "outputId": "7cfbe3f8-fff0-4355-b072-f73c37e4159a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力 X: tensor([[ 0.9023,  0.3733,  0.3107,  0.7211],\n",
            "        [ 1.9552,  0.5846,  1.4028,  0.3647],\n",
            "        [ 0.7259,  0.8475, -0.3782,  0.9767]])\n",
            "出力 out: tensor([[ 0.1082, -1.8860,  2.0069,  1.5440],\n",
            "        [ 1.7865, -1.2831,  3.5608, -0.2731],\n",
            "        [-0.3764, -0.6505, -0.1868,  2.8405]], grad_fn=<AddBackward0>)\n",
            "shape: torch.Size([3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_v = d_model // num_heads\n",
        "        # 線形変換（Q,K,V)で全部まとめて作る\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape # Batch, token長, d_model\n",
        "\n",
        "        Q = self.W_Q(x)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        Q = Q.view(B,T,num_heads,d_k).transpose(1,2)\n",
        "        K = K.view(B,T,num_heads, d_k),transpose(1,2)\n",
        "        V = V.view(B,T,num_heads, d_v).transpose(1,2)\n",
        "        K = K // d_k\n",
        "        V = V // d_v\n",
        "\n",
        "        scores = Q @ K.transpose(-1, -2) / math.sqrt(self.d_k)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(B,T,D)\n",
        "\n",
        "        out = self.W_O(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "qYCrYOQa1zWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_v = d_model // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        Q = self.W_Q(x)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        # head分割\n",
        "        Q = Q.view(B,T,self.num_heads, self.d_k)\n",
        "        K = K.view(B, T, self.num_heads, self.d_k)\n",
        "        V = V.view(B, T, self.num_heads, self.d_v)\n",
        "\n",
        "        scores = Q @ K.transpose(-1,-2) / math.sqrt(self.d_k)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "\n",
        "        # ヘッドの結合\n",
        "        out = out.transpose(1,2).contiguous().view(B,T,D)\n",
        "\n",
        "        # 出力projection\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "R9B9nZLQOz4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model=4, num_heads=2)\n",
        "x = torch.randn(1, 3, 4)\n",
        "out = mha(x)\n",
        "\n",
        "print(\"入力:\", x)\n",
        "print(\"出力:\", out)\n",
        "print(\"shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iSwvJnAQyHV",
        "outputId": "27378e4a-6417-4983-8926-64371f39e371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力: tensor([[[-1.3003, -1.8968,  1.7726,  0.7622],\n",
            "         [ 0.8025,  0.5793, -0.3923,  0.2900],\n",
            "         [ 0.2742, -0.1099, -0.2101,  2.7433]]])\n",
            "出力: tensor([[[-0.4964,  0.6343, -0.0478,  0.3425],\n",
            "         [-1.1541,  0.4896, -0.2930,  0.3507],\n",
            "         [-0.2134,  0.2677, -0.4627, -0.2826]]], grad_fn=<ViewBackward0>)\n",
            "shape: torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MiniTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_hidden):\n",
        "        super().__init__()\n",
        "\n",
        "        # Q,K,Vの線形層\n",
        "        self.W_Q = nn.Linear(d_model,d_k)\n",
        "        self.W_K = nn.Linear(d_model,d_k)\n",
        "        self.W_V = nn.Linear(d_model,d_v)\n",
        "\n",
        "        #出力 projection\n",
        "        self.W_O = nn.Linear(d_v, d_model)\n",
        "\n",
        "        # LayerNorm\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # FFN\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.GELU(), # ReLUより滑らかな活性化関数\n",
        "            nn.Linear(d_hidden, d_model)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. LayerNorm\n",
        "        h = self.ln1(x)\n",
        "        attn_out = self.mha(h)\n",
        "\n",
        "        # 2. Residual 1\n",
        "        x = x + out\n",
        "\n",
        "        # 3. LayerNorm -> FFN\n",
        "        h2 = self.ln2(x)\n",
        "        ffn_out = self.ffn(h2)\n",
        "\n",
        "        # 4. residual 2\n",
        "        x = x + h2\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "cpqFbdUSQzMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = MiniTransformerBlock(d_model=4, num_heads=2, d_hidden=8)\n",
        "\n",
        "X = torch.randn(1, 3, 4)\n",
        "out = block(X)\n",
        "\n",
        "print(\"入力:\", X)\n",
        "print(\"出力:\", out)\n",
        "print(\"shape:\", out.shape)\n"
      ],
      "metadata": {
        "id": "CTQMV0IoTAHo",
        "outputId": "6fb6dd0c-32ac-49d5-9a82-87aceffb1e48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力: tensor([[[-0.8717,  0.7862,  0.0088,  1.2842],\n",
            "         [-0.8768, -1.0473, -1.0807, -0.9000],\n",
            "         [-0.9916,  0.1127,  1.7978, -1.5221]]])\n",
            "出力: tensor([[[-2.8367,  2.2549, -0.4098,  2.6315],\n",
            "         [-3.4882,  0.3624, -1.7703,  0.3845],\n",
            "         [-1.9120,  0.9449,  2.6656, -2.9927]]], grad_fn=<AddBackward0>)\n",
            "shape: torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6T2FQ-RcTAdr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}